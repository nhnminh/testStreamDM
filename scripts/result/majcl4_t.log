Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/07 00:37:15 INFO SparkContext: Running Spark version 1.5.1
17/09/07 00:37:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/07 00:37:16 INFO SecurityManager: Changing view acls to: minhnguyen
17/09/07 00:37:16 INFO SecurityManager: Changing modify acls to: minhnguyen
17/09/07 00:37:16 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(minhnguyen); users with modify permissions: Set(minhnguyen)
17/09/07 00:37:17 INFO Slf4jLogger: Slf4jLogger started
17/09/07 00:37:17 INFO Remoting: Starting remoting
17/09/07 00:37:17 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@192.168.0.102:49323]
17/09/07 00:37:17 INFO Utils: Successfully started service 'sparkDriver' on port 49323.
17/09/07 00:37:17 INFO SparkEnv: Registering MapOutputTracker
17/09/07 00:37:17 INFO SparkEnv: Registering BlockManagerMaster
17/09/07 00:37:17 INFO DiskBlockManager: Created local directory at /private/var/folders/5d/zfph4vmx069g8444mbjrg03r0000gn/T/blockmgr-18b2e3ac-397b-401b-9fca-361c15407a58
17/09/07 00:37:17 INFO MemoryStore: MemoryStore started with capacity 2.1 GB
17/09/07 00:37:17 INFO HttpFileServer: HTTP File server directory is /private/var/folders/5d/zfph4vmx069g8444mbjrg03r0000gn/T/spark-0c1ebae1-b884-4f4f-a37d-59ff32ee0cce/httpd-e51a72cc-a88f-4d4d-a499-db7822fb215f
17/09/07 00:37:17 INFO HttpServer: Starting HTTP Server
17/09/07 00:37:17 INFO Utils: Successfully started service 'HTTP file server' on port 49324.
17/09/07 00:37:17 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/07 00:37:17 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/07 00:37:17 INFO SparkUI: Started SparkUI at http://192.168.0.102:4040
17/09/07 00:37:17 INFO SparkContext: Added JAR file:/Users/minhnguyen/StreamingAlgo/StreamDM/streamDM/scripts/../target/scala-2.10/streamdm-spark-streaming_2.10-0.2.jar at http://192.168.0.102:49324/jars/streamdm-spark-streaming_2.10-0.2.jar with timestamp 1504737437994
17/09/07 00:37:18 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
17/09/07 00:37:18 INFO Executor: Starting executor ID driver on host localhost
17/09/07 00:37:18 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49325.
17/09/07 00:37:18 INFO NettyBlockTransferService: Server created on 49325
17/09/07 00:37:18 INFO BlockManagerMaster: Trying to register BlockManager
17/09/07 00:37:18 INFO BlockManagerMasterEndpoint: Registering block manager localhost:49325 with 2.1 GB RAM, BlockManagerId(driver, localhost, 49325)
17/09/07 00:37:18 INFO BlockManagerMaster: Registered BlockManager
17/09/07 00:37:18 INFO FileReader: 1
17/09/07 00:37:18 INFO FileReader: 0
17/09/07 00:37:18 INFO HoeffdingTreeModel: numClasses2
17/09/07 00:37:19 INFO ForEachDStream: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO FileReader$$anon$1: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO FileReader$$anon$1: Slide time = 10 ms
17/09/07 00:37:19 INFO FileReader$$anon$1: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO FileReader$$anon$1: Checkpoint interval = null
17/09/07 00:37:19 INFO FileReader$$anon$1: Remember duration = 10 ms
17/09/07 00:37:19 INFO FileReader$$anon$1: Initialized and validated org.apache.spark.streamdm.streams.FileReader$$anon$1@59ff02a8
17/09/07 00:37:19 INFO ForEachDStream: Slide time = 10 ms
17/09/07 00:37:19 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO ForEachDStream: Checkpoint interval = null
17/09/07 00:37:19 INFO ForEachDStream: Remember duration = 10 ms
17/09/07 00:37:19 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@6faa3057
17/09/07 00:37:19 INFO ForEachDStream: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO MappedDStream: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO MappedDStream: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO ShuffledDStream: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO MappedDStream: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO MappedDStream: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO MappedDStream: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO FileReader$$anon$1: metadataCleanupDelay = -1
17/09/07 00:37:19 INFO FileReader$$anon$1: Slide time = 10 ms
17/09/07 00:37:19 INFO FileReader$$anon$1: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO FileReader$$anon$1: Checkpoint interval = null
17/09/07 00:37:19 INFO FileReader$$anon$1: Remember duration = 10 ms
17/09/07 00:37:19 INFO FileReader$$anon$1: Initialized and validated org.apache.spark.streamdm.streams.FileReader$$anon$1@59ff02a8
17/09/07 00:37:19 INFO MappedDStream: Slide time = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO MappedDStream: Checkpoint interval = null
17/09/07 00:37:19 INFO MappedDStream: Remember duration = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@67b6a0ab
17/09/07 00:37:19 INFO MappedDStream: Slide time = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO MappedDStream: Checkpoint interval = null
17/09/07 00:37:19 INFO MappedDStream: Remember duration = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@13deeeb0
17/09/07 00:37:19 INFO MappedDStream: Slide time = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO MappedDStream: Checkpoint interval = null
17/09/07 00:37:19 INFO MappedDStream: Remember duration = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@32954421
17/09/07 00:37:19 INFO ShuffledDStream: Slide time = 10 ms
17/09/07 00:37:19 INFO ShuffledDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO ShuffledDStream: Checkpoint interval = null
17/09/07 00:37:19 INFO ShuffledDStream: Remember duration = 10 ms
17/09/07 00:37:19 INFO ShuffledDStream: Initialized and validated org.apache.spark.streaming.dstream.ShuffledDStream@c83f2ac
17/09/07 00:37:19 INFO MappedDStream: Slide time = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO MappedDStream: Checkpoint interval = null
17/09/07 00:37:19 INFO MappedDStream: Remember duration = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@5e62a86f
17/09/07 00:37:19 INFO MappedDStream: Slide time = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO MappedDStream: Checkpoint interval = null
17/09/07 00:37:19 INFO MappedDStream: Remember duration = 10 ms
17/09/07 00:37:19 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@3a2d767d
17/09/07 00:37:19 INFO ForEachDStream: Slide time = 10 ms
17/09/07 00:37:19 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
17/09/07 00:37:19 INFO ForEachDStream: Checkpoint interval = null
17/09/07 00:37:19 INFO ForEachDStream: Remember duration = 10 ms
17/09/07 00:37:19 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@22818955
17/09/07 00:37:19 INFO RecurringTimer: Started timer for JobGenerator at time 1504737439380
17/09/07 00:37:19 INFO JobGenerator: Started JobGenerator at 1504737439380 ms
17/09/07 00:37:19 INFO JobScheduler: Started JobScheduler
17/09/07 00:37:19 INFO StreamingContext: StreamingContext started
17/09/07 00:37:19 INFO JobScheduler: Added jobs for time 1504737439380 ms
17/09/07 00:37:19 INFO JobScheduler: Starting job streaming job 1504737439380 ms.0 from job set of time 1504737439380 ms
17/09/07 00:37:19 INFO HoeffdingTreeModel: numClasses2
17/09/07 00:37:19 INFO JobScheduler: Finished job streaming job 1504737439380 ms.0 from job set of time 1504737439380 ms
17/09/07 00:37:19 INFO JobScheduler: Starting job streaming job 1504737439380 ms.1 from job set of time 1504737439380 ms
17/09/07 00:37:19 ERROR JobScheduler: Error running job streaming job 1504737439380 ms.0
org.apache.spark.SparkException: Task not serializable
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2030)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1075)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1071)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1.apply(HoeffdingTree.scala:162)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1.apply(HoeffdingTree.scala:161)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.NotSerializableException: org.apache.spark.streaming.StreamingContext
Serialization stack:
	- object not serializable (class: org.apache.spark.streaming.StreamingContext, value: org.apache.spark.streaming.StreamingContext@5b1301b6)
	- field (class: org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1, name: ssc$1, type: class org.apache.spark.streaming.StreamingContext)
	- object (class org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1, <function1>)
	- field (class: org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$2, name: $outer, type: class org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1)
	- object (class org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$2, <function2>)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)
	... 29 more
Exception in thread "main" org.apache.spark.SparkException: Task not serializable
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2030)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1075)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1071)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1.apply(HoeffdingTree.scala:162)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1.apply(HoeffdingTree.scala:161)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.NotSerializableException: org.apache.spark.streaming.StreamingContext
Serialization stack:
	- object not serializable (class: org.apache.spark.streaming.StreamingContext, value: org.apache.spark.streaming.StreamingContext@5b1301b6)
	- field (class: org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1, name: ssc$1, type: class org.apache.spark.streaming.StreamingContext)
	- object (class org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1, <function1>)
	- field (class: org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$2, name: $outer, type: class org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1)
	- object (class org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$2, <function2>)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)
	... 29 more
17/09/07 00:37:19 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
17/09/07 00:37:19 INFO JobGenerator: Stopping JobGenerator immediately
17/09/07 00:37:19 INFO RecurringTimer: Stopped timer for JobGenerator after time 1504737439520
17/09/07 00:37:19 INFO JobScheduler: Added jobs for time 1504737439520 ms
17/09/07 00:37:19 INFO JobGenerator: Stopped JobGenerator
17/09/07 00:37:19 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:-2
17/09/07 00:37:19 INFO DAGScheduler: Registering RDD 3 (main at NativeMethodAccessorImpl.java:-2)
17/09/07 00:37:19 INFO DAGScheduler: Got job 0 (main at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/09/07 00:37:19 INFO DAGScheduler: Final stage: ResultStage 1(main at NativeMethodAccessorImpl.java:-2)
17/09/07 00:37:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
17/09/07 00:37:19 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
17/09/07 00:37:19 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at main at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/09/07 00:37:19 INFO MemoryStore: ensureFreeSpace(11224) called with curMem=0, maxMem=2222739947
17/09/07 00:37:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 11.0 KB, free 2.1 GB)
17/09/07 00:37:19 INFO MemoryStore: ensureFreeSpace(5170) called with curMem=11224, maxMem=2222739947
17/09/07 00:37:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KB, free 2.1 GB)
17/09/07 00:37:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:49325 (size: 5.0 KB, free: 2.1 GB)
17/09/07 00:37:19 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:861
17/09/07 00:37:19 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at main at NativeMethodAccessorImpl.java:-2)
17/09/07 00:37:19 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
17/09/07 00:37:19 WARN TaskSetManager: Stage 0 contains a task of very large size (252 KB). The maximum recommended task size is 100 KB.
17/09/07 00:37:19 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 258395 bytes)
17/09/07 00:37:19 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 258395 bytes)
17/09/07 00:37:19 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/09/07 00:37:19 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)
17/09/07 00:37:19 INFO Executor: Fetching http://192.168.0.102:49324/jars/streamdm-spark-streaming_2.10-0.2.jar with timestamp 1504737437994
17/09/07 00:37:20 INFO Utils: Fetching http://192.168.0.102:49324/jars/streamdm-spark-streaming_2.10-0.2.jar to /private/var/folders/5d/zfph4vmx069g8444mbjrg03r0000gn/T/spark-0c1ebae1-b884-4f4f-a37d-59ff32ee0cce/userFiles-cca8a0f7-aac0-4e0b-bfa6-9a638d9a472c/fetchFileTemp906902484177480735.tmp
17/09/07 00:37:20 INFO Executor: Adding file:/private/var/folders/5d/zfph4vmx069g8444mbjrg03r0000gn/T/spark-0c1ebae1-b884-4f4f-a37d-59ff32ee0cce/userFiles-cca8a0f7-aac0-4e0b-bfa6-9a638d9a472c/streamdm-spark-streaming_2.10-0.2.jar to class loader
17/09/07 00:37:20 INFO Executor: Finished task 1.0 in stage 0.0 (TID 1). 1158 bytes result sent to driver
17/09/07 00:37:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1158 bytes result sent to driver
17/09/07 00:37:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 386 ms on localhost (1/2)
17/09/07 00:37:20 INFO TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 355 ms on localhost (2/2)
17/09/07 00:37:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/09/07 00:37:20 INFO DAGScheduler: ShuffleMapStage 0 (main at NativeMethodAccessorImpl.java:-2) finished in 0.408 s
17/09/07 00:37:20 INFO DAGScheduler: looking for newly runnable stages
17/09/07 00:37:20 INFO DAGScheduler: running: Set()
17/09/07 00:37:20 INFO DAGScheduler: waiting: Set(ResultStage 1)
17/09/07 00:37:20 INFO DAGScheduler: failed: Set()
17/09/07 00:37:20 INFO DAGScheduler: Missing parents for ResultStage 1: List()
17/09/07 00:37:20 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at main at NativeMethodAccessorImpl.java:-2), which is now runnable
17/09/07 00:37:20 INFO MemoryStore: ensureFreeSpace(3376) called with curMem=16394, maxMem=2222739947
17/09/07 00:37:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.3 KB, free 2.1 GB)
17/09/07 00:37:20 INFO MemoryStore: ensureFreeSpace(1920) called with curMem=19770, maxMem=2222739947
17/09/07 00:37:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1920.0 B, free 2.1 GB)
17/09/07 00:37:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:49325 (size: 1920.0 B, free: 2.1 GB)
17/09/07 00:37:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861
17/09/07 00:37:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at main at NativeMethodAccessorImpl.java:-2)
17/09/07 00:37:20 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/09/07 00:37:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, PROCESS_LOCAL, 1980 bytes)
17/09/07 00:37:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)
17/09/07 00:37:20 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/09/07 00:37:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 6 ms
17/09/07 00:37:20 INFO Executor: Finished task 0.0 in stage 1.0 (TID 2). 1185 bytes result sent to driver
17/09/07 00:37:20 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 42 ms on localhost (1/1)
17/09/07 00:37:20 INFO DAGScheduler: ResultStage 1 (main at NativeMethodAccessorImpl.java:-2) finished in 0.042 s
17/09/07 00:37:20 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/09/07 00:37:20 INFO DAGScheduler: Job 0 finished: main at NativeMethodAccessorImpl.java:-2, took 0.679684 s
17/09/07 00:37:20 INFO JobScheduler: Finished job streaming job 1504737439380 ms.1 from job set of time 1504737439380 ms
17/09/07 00:37:20 INFO HoeffdingTreeModel: numClasses2
17/09/07 00:37:20 INFO JobScheduler: Total delay: 0.970 s for time 1504737439380 ms (execution: 0.823 s)
17/09/07 00:37:20 INFO JobScheduler: Starting job streaming job 1504737439520 ms.0 from job set of time 1504737439520 ms
17/09/07 00:37:20 INFO JobScheduler: Finished job streaming job 1504737439520 ms.0 from job set of time 1504737439520 ms
17/09/07 00:37:20 INFO JobScheduler: Starting job streaming job 1504737439520 ms.1 from job set of time 1504737439520 ms
17/09/07 00:37:20 ERROR JobScheduler: Error running job streaming job 1504737439520 ms.0
org.apache.spark.SparkException: Task not serializable
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2030)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1075)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1071)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1.apply(HoeffdingTree.scala:162)
	at org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1.apply(HoeffdingTree.scala:161)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.NotSerializableException: org.apache.spark.streaming.StreamingContext
Serialization stack:
	- object not serializable (class: org.apache.spark.streaming.StreamingContext, value: org.apache.spark.streaming.StreamingContext@5b1301b6)
	- field (class: org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1, name: ssc$1, type: class org.apache.spark.streaming.StreamingContext)
	- object (class org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1, <function1>)
	- field (class: org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$2, name: $outer, type: class org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1)
	- object (class org.apache.spark.streamdm.classifiers.trees.HoeffdingTree$$anonfun$train$1$$anonfun$2, <function2>)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)
	... 29 more
17/09/07 00:37:20 INFO SparkContext: Starting job: main at NativeMethodAccessorImpl.java:-2
17/09/07 00:37:20 INFO DAGScheduler: Registering RDD 10 (main at NativeMethodAccessorImpl.java:-2)
17/09/07 00:37:20 INFO DAGScheduler: Got job 1 (main at NativeMethodAccessorImpl.java:-2) with 1 output partitions
17/09/07 00:37:20 INFO DAGScheduler: Final stage: ResultStage 3(main at NativeMethodAccessorImpl.java:-2)
17/09/07 00:37:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
17/09/07 00:37:20 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
17/09/07 00:37:20 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at main at NativeMethodAccessorImpl.java:-2), which has no missing parents
17/09/07 00:37:20 INFO MemoryStore: ensureFreeSpace(11224) called with curMem=21690, maxMem=2222739947
17/09/07 00:37:20 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 11.0 KB, free 2.1 GB)
17/09/07 00:37:20 INFO MemoryStore: ensureFreeSpace(5167) called with curMem=32914, maxMem=2222739947
17/09/07 00:37:20 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 5.0 KB, free 2.1 GB)
17/09/07 00:37:20 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:49325 (size: 5.0 KB, free: 2.1 GB)
17/09/07 00:37:20 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861
17/09/07 00:37:20 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at main at NativeMethodAccessorImpl.java:-2)
17/09/07 00:37:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks
17/09/07 00:37:20 WARN TaskSetManager: Stage 2 contains a task of very large size (252 KB). The maximum recommended task size is 100 KB.
17/09/07 00:37:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, localhost, PROCESS_LOCAL, 258395 bytes)
17/09/07 00:37:20 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, localhost, PROCESS_LOCAL, 258395 bytes)
17/09/07 00:37:20 INFO Executor: Running task 1.0 in stage 2.0 (TID 4)
17/09/07 00:37:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 3)
17/09/07 00:37:20 INFO Executor: Finished task 1.0 in stage 2.0 (TID 4). 1158 bytes result sent to driver
17/09/07 00:37:20 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 53 ms on localhost (1/2)
17/09/07 00:37:20 INFO Executor: Finished task 0.0 in stage 2.0 (TID 3). 1158 bytes result sent to driver
17/09/07 00:37:20 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 70 ms on localhost (2/2)
17/09/07 00:37:20 INFO DAGScheduler: ShuffleMapStage 2 (main at NativeMethodAccessorImpl.java:-2) finished in 0.070 s
17/09/07 00:37:20 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/09/07 00:37:20 INFO DAGScheduler: looking for newly runnable stages
17/09/07 00:37:20 INFO DAGScheduler: running: Set()
17/09/07 00:37:20 INFO DAGScheduler: waiting: Set(ResultStage 3)
17/09/07 00:37:20 INFO DAGScheduler: failed: Set()
17/09/07 00:37:20 INFO DAGScheduler: Missing parents for ResultStage 3: List()
17/09/07 00:37:20 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at main at NativeMethodAccessorImpl.java:-2), which is now runnable
17/09/07 00:37:20 INFO MemoryStore: ensureFreeSpace(3376) called with curMem=38081, maxMem=2222739947
17/09/07 00:37:20 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 3.3 KB, free 2.1 GB)
17/09/07 00:37:20 INFO MemoryStore: ensureFreeSpace(1908) called with curMem=41457, maxMem=2222739947
17/09/07 00:37:20 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1908.0 B, free 2.1 GB)
17/09/07 00:37:20 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:49325 (size: 1908.0 B, free: 2.1 GB)
17/09/07 00:37:20 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861
17/09/07 00:37:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at main at NativeMethodAccessorImpl.java:-2)
17/09/07 00:37:20 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
17/09/07 00:37:20 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 5, localhost, PROCESS_LOCAL, 1980 bytes)
17/09/07 00:37:20 INFO Executor: Running task 0.0 in stage 3.0 (TID 5)
17/09/07 00:37:20 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/09/07 00:37:20 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/09/07 00:37:20 INFO Executor: Finished task 0.0 in stage 3.0 (TID 5). 1185 bytes result sent to driver
17/09/07 00:37:20 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 5) in 6 ms on localhost (1/1)
17/09/07 00:37:20 INFO DAGScheduler: ResultStage 3 (main at NativeMethodAccessorImpl.java:-2) finished in 0.006 s
17/09/07 00:37:20 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/09/07 00:37:20 INFO DAGScheduler: Job 1 finished: main at NativeMethodAccessorImpl.java:-2, took 0.097334 s
17/09/07 00:37:20 INFO JobScheduler: Finished job streaming job 1504737439520 ms.1 from job set of time 1504737439520 ms
17/09/07 00:37:20 INFO JobScheduler: Total delay: 0.942 s for time 1504737439520 ms (execution: 0.108 s)
17/09/07 00:37:20 ERROR StreamingListenerBus: StreamingListenerBus has already stopped! Dropping event StreamingListenerBatchCompleted(BatchInfo(1504737439520 ms,Map(),1504737439623,Some(1504737440354),Some(1504737440462)))
17/09/07 00:37:20 INFO JobScheduler: Stopped JobScheduler
17/09/07 00:37:20 INFO StreamingContext: StreamingContext stopped successfully
17/09/07 00:37:20 INFO SparkContext: Invoking stop() from shutdown hook
17/09/07 00:37:20 INFO SparkUI: Stopped Spark web UI at http://192.168.0.102:4040
17/09/07 00:37:20 INFO DAGScheduler: Stopping DAGScheduler
17/09/07 00:37:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/09/07 00:37:20 INFO MemoryStore: MemoryStore cleared
17/09/07 00:37:20 INFO BlockManager: BlockManager stopped
17/09/07 00:37:20 INFO BlockManagerMaster: BlockManagerMaster stopped
17/09/07 00:37:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/09/07 00:37:20 INFO SparkContext: Successfully stopped SparkContext
17/09/07 00:37:20 INFO ShutdownHookManager: Shutdown hook called
17/09/07 00:37:20 INFO ShutdownHookManager: Deleting directory /private/var/folders/5d/zfph4vmx069g8444mbjrg03r0000gn/T/spark-0c1ebae1-b884-4f4f-a37d-59ff32ee0cce
